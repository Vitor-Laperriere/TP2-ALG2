{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP0flYrhna18"
      },
      "source": [
        "### Trabalho Prático 2 – Soluções para problemas difíceis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxefDJ3fzlwp"
      },
      "source": [
        "#### Importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "C6zcvh1Tzfg1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import time\n",
        "import warnings\n",
        "from itertools import cycle, islice\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK4ILlacWK0X",
        "outputId": "9e18c71e-670a-4d69-b612-79715ef3ba74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.1.4)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.7.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFt7HQINnndy"
      },
      "source": [
        " #### Distância de Minkowski"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DbrRbCsDnWA_"
      },
      "outputs": [],
      "source": [
        "# p = 1 (Distância Manhattan)\n",
        "# p = 2 (Distância Euclidiana)\n",
        "\n",
        "def minkowski(x, y, p):\n",
        "   return np.power(np.sum(np.abs(x - y) ** p), 1/p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XPLqlEDepl_L"
      },
      "outputs": [],
      "source": [
        "def matriz_dist(df, p):\n",
        "    npontos = df.shape[0]\n",
        "    distancias = np.zeros((npontos, npontos))\n",
        "\n",
        "    for i in range(npontos):\n",
        "        for j in range(npontos):\n",
        "            ponto1 = df.iloc[i].values\n",
        "            ponto2 = df.iloc[j].values\n",
        "            distancias[i, j] = minkowski(ponto1, ponto2, p)\n",
        "\n",
        "    return distancias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-9oFwseor37"
      },
      "source": [
        "#### Algoritmos aproximados K-Centros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idgw2Hyq0lre"
      },
      "source": [
        "##### Algoritmo baseado em refinamento de intervalos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "10hkMC4q0rer"
      },
      "outputs": [],
      "source": [
        "def rvalid(npontos, raio, distancias, k):\n",
        "  s_aux = np.arange(npontos)\n",
        "  C = []\n",
        "  while len(s_aux)>0:\n",
        "    C.append(s_aux[0])\n",
        "    s_aux = s_aux[1:]\n",
        "    s_aux = [x for x in s_aux if distancias[C[-1], x] > 2*raio]\n",
        "  return len(C) <= k, C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wR7UzLasXvoc"
      },
      "outputs": [],
      "source": [
        "def calculo_rmax(distancias, npontos):\n",
        "  rmax = -1\n",
        "  for i in range(npontos):\n",
        "    for j in range(npontos):\n",
        "      rmax = max(rmax, distancias[i, j])\n",
        "\n",
        "  return rmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BxkLtHOcmdA2"
      },
      "outputs": [],
      "source": [
        "def kcentros1(npontos, distancias,k, ref):\n",
        "  rmax = calculo_rmax(distancias, npontos)\n",
        "  l = 0;\n",
        "  r = rmax;\n",
        "\n",
        "  while((r-l)/rmax > ref):\n",
        "    m = (l+r)/2.0\n",
        "    v, C = rvalid(npontos, m, distancias, k)\n",
        "    if(v):\n",
        "      r = m\n",
        "    else:\n",
        "      l = m+1\n",
        "  while (l != r):\n",
        "    m = (l+r)/2\n",
        "    v, C = rvalid(npontos, m, distancias, k)\n",
        "    if(v):\n",
        "      return C\n",
        "    else:\n",
        "      l = m+1\n",
        "  v, C = rvalid(npontos, l, distancias, k)\n",
        "  return C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpHibAopTYgx"
      },
      "source": [
        "Os intervalos considerados serão de 5%, 10%, 15%, 20% e 25%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vde577VGGWbU"
      },
      "source": [
        "#### Algoritmo baseado na maximização da distância entre os centros previamente escolhidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SVGtvmBro3L7"
      },
      "outputs": [],
      "source": [
        "def kcentros2(k, npontos, distancias):\n",
        "  if(k > npontos):\n",
        "    return np.arange()\n",
        "  C = [0]\n",
        "  idponto = 0\n",
        "  while(len(C) < k):\n",
        "    dist = -1\n",
        "    for c in C:\n",
        "      for i in range(npontos):\n",
        "        if(distancias[i,c] > dist):\n",
        "          idponto = i\n",
        "          dist = distancias[i, c]\n",
        "    C.append(idponto)\n",
        "  return C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh6o9W1U5H2O"
      },
      "source": [
        "#### Define o indice do cluster para cada um dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RuUoP7Y74rjp"
      },
      "outputs": [],
      "source": [
        "def cluster_dados(pontos, centros, distancias):\n",
        "  labels = []\n",
        "  for i in range(pontos.shape[0]):\n",
        "    dist_centro = float('inf')\n",
        "    id_cluster = 0\n",
        "    for c in centros:\n",
        "      if(distancias[i, c] < dist_centro):\n",
        "        dist_centro = distancias[i, c]\n",
        "        id_cluster = c\n",
        "    labels.append(id_cluster)\n",
        "\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4c1y01ot1kx"
      },
      "source": [
        "#### Cálculo do raio da solução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ciBf5FsNuBGz"
      },
      "outputs": [],
      "source": [
        "def raio(labels, distancias):\n",
        "  raio = -1\n",
        "  for i in range(len(labels)):\n",
        "    raio = max(raio, distancias[i,labels[i]])\n",
        "\n",
        "  return raio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7wiagZkg542"
      },
      "source": [
        "#### Experimentos com o kcentros1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "L3kwPEZ5s7C5"
      },
      "outputs": [],
      "source": [
        "def experimentos_kcentros1(m_dist, n_pontos,  X, y_map ,t_dist, id_dataset, k):\n",
        "  ref = [0.05, 0.1, 0.15, 0.2, 0.25]\n",
        "  for i in range(5):\n",
        "    tmp_exec_kmeans = []\n",
        "    silhueta_kmeans = []\n",
        "    indice_rand_kmeans = []\n",
        "    raio_exec_kmeans = []\n",
        "\n",
        "    for j in range(30):\n",
        "      start_time = time.time()\n",
        "\n",
        "      C = kcentros1(n_pontos, m_dist, k, ref[i])\n",
        "\n",
        "      end_time = time.time()\n",
        "      execution_time = end_time - start_time\n",
        "\n",
        "      c_dados = cluster_dados(X, C, m_dist)\n",
        "\n",
        "      tmp_exec_kmeans.append(execution_time)\n",
        "\n",
        "\n",
        "      if len(np.unique(c_dados)) > 1:\n",
        "        silhueta_kmeans.append(silhouette_score(X, c_dados))\n",
        "      else:\n",
        "        silhueta_kmeans.append(-1)\n",
        "\n",
        "      indice_rand_kmeans.append(adjusted_rand_score(y_map, c_dados))\n",
        "\n",
        "      raio_exec_kmeans.append(raio(c_dados, m_dist))\n",
        "\n",
        "    # Retornar as médias e desvios-padrão das métricas\n",
        "    data = {\n",
        "        'Algoritmo': 'K-Centros 1',\n",
        "        'Tempo Execução (s)': f\"{np.mean(tmp_exec_kmeans):.4f} ± {np.std(tmp_exec_kmeans):.4f}\",\n",
        "        'Silhueta': f\"{np.mean(silhueta_kmeans):.4f} ± {np.std(silhueta_kmeans):.4f}\",\n",
        "        'Índice de Rand Ajustado': f\"{np.mean(indice_rand_kmeans):.4f} ± {np.std(indice_rand_kmeans):.4f}\",\n",
        "        'Raio Máximo': f\"{np.mean(raio_exec_kmeans):.4f} ± {np.std(raio_exec_kmeans):.4f}\"\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data, index=[0])\n",
        "    df.to_csv(f'dataset_exec_kcentros1_{t_dist}_{[ref[i]]}_dataset_{id_dataset}.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV0nfiHUhBXR"
      },
      "source": [
        "#### Experimentos com o kcentros2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4o_umZj-hAOr"
      },
      "outputs": [],
      "source": [
        "def experimentos_kcentros2(m_dist, npontos, X, y_map, t_dist, id_dataset, k):\n",
        "    tmp_exec_kmeans = []\n",
        "    silhueta_kmeans = []\n",
        "    indice_rand_kmeans = []\n",
        "    raio_exec_kmeans = []\n",
        "\n",
        "    for i in range(30):\n",
        "      start_time = time.time()\n",
        "\n",
        "      C = kcentros2(k, npontos, m_dist)\n",
        "\n",
        "      end_time = time.time()\n",
        "      execution_time = end_time - start_time\n",
        "\n",
        "      c_dados = cluster_dados(X, C, m_dist)\n",
        "\n",
        "      tmp_exec_kmeans.append(execution_time)\n",
        "      silhueta_kmeans.append(silhouette_score(X, c_dados))\n",
        "      indice_rand_kmeans.append(adjusted_rand_score(y_map, c_dados))\n",
        "\n",
        "      raio_exec_kmeans.append(raio(c_dados, m_dist))\n",
        "\n",
        "    data = {\n",
        "        'Algoritmo': 'K-Centros 2',\n",
        "        'Tempo Execução (s)': f\"{np.mean(tmp_exec_kmeans):.4f} ± {np.std(tmp_exec_kmeans):.4f}\",\n",
        "        'Silhueta': f\"{np.mean(silhueta_kmeans):.4f} ± {np.std(silhueta_kmeans):.4f}\",\n",
        "        'Índice de Rand Ajustado': f\"{np.mean(indice_rand_kmeans):.4f} ± {np.std(indice_rand_kmeans):.4f}\",\n",
        "        'Raio Máximo': f\"{np.mean(raio_exec_kmeans):.4f} ± {np.std(raio_exec_kmeans):.4f}\"\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data, index=[0])\n",
        "    df.to_csv(f'dataset_exec_kcentros2_{t_dist}_dataset_{id_dataset}.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def carregar_datasets(path, num_datasets, sintetico):\n",
        "    datasets = []\n",
        "    for i in range(num_datasets):\n",
        "        if(sintetico ==1):\n",
        "          filename = os.path.join(path, f'dataset_{i}.csv')\n",
        "        elif(sintetico ==2):\n",
        "          filename = os.path.join(path, f'dataset_{i}_dist_mult.csv')\n",
        "\n",
        "        df = pd.read_csv(filename)\n",
        "        datasets.append(df)\n",
        "    return datasets\n",
        "\n",
        "def processar_datasets(datasets, sintetico):\n",
        "    resultados = []\n",
        "    for i, df in enumerate(datasets):\n",
        "\n",
        "      if(sintetico ==1):\n",
        "        X = df[['feature_1', 'feature_2']]\n",
        "        y = df['target']\n",
        "\n",
        "      elif(sintetico ==2):\n",
        "        X = df[['X1', 'X2']]\n",
        "        y = df['Center']\n",
        "\n",
        "      k = y.nunique()\n",
        "\n",
        "      distancia_euclidiana =  matriz_dist(X,2)\n",
        "      distancia_manhattan =  matriz_dist(X,1)\n",
        "\n",
        "      experimentos_kcentros2(distancia_euclidiana, X.shape[0], X, y, 'Euclidiana', i, k)\n",
        "      experimentos_kcentros1(distancia_manhattan, X.shape[0], X, y, 'Manhattan', i, k)\n",
        "\n"
      ],
      "metadata": {
        "id": "cpoFjswmXo0M"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Sinteticos 1\n",
        "datasets = carregar_datasets('.', 10, 1)\n",
        "processar_datasets(datasets, 1)\n",
        "\n",
        "##Sinteticos 2\n",
        "datasets = carregar_datasets('.', 10, 2)\n",
        "processar_datasets(datasets, 2)\n"
      ],
      "metadata": {
        "id": "MBQu6yirXuIQ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3_K6w3IRX9tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-LRiOq0rMHB"
      },
      "source": [
        "#### Avaliação algoritmos aproximados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSuB2YvEaLnE"
      },
      "source": [
        "##### Dados sintéticos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rx0q-ltlVEX"
      },
      "source": [
        "**Abordagem do scikit-learn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "13Q54B90VV3Y",
        "outputId": "da489bf4-64ce-498f-fa39-a37e6df120b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 2100x1300 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import time\n",
        "import warnings\n",
        "from itertools import cycle, islice\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import cluster, datasets, mixture\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ============\n",
        "# Generate datasets. We choose the size big enough to see the scalability\n",
        "# of the algorithms, but not too big to avoid too long running times\n",
        "# ============\n",
        "n_samples = 700\n",
        "seed = 30\n",
        "noisy_circles = datasets.make_circles(\n",
        "    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n",
        ")\n",
        "circles = datasets.make_circles(\n",
        "    n_samples=n_samples, factor=0.9, random_state=seed\n",
        ")\n",
        "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\n",
        "noisy_moons_2 = datasets.make_moons(n_samples=n_samples, noise=0.09, random_state=seed)\n",
        "blobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\n",
        "blobs_2 = datasets.make_blobs(n_samples=n_samples, random_state=40)\n",
        "rng = np.random.RandomState(seed)\n",
        "no_structure = rng.rand(n_samples, 2), None\n",
        "\n",
        "# Anisotropicly distributed data\n",
        "random_state = 170\n",
        "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
        "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "X_aniso = np.dot(X, transformation)\n",
        "aniso = (X_aniso, y)\n",
        "\n",
        "# blobs with varied variances\n",
        "varied = datasets.make_blobs(\n",
        "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
        ")\n",
        "\n",
        "varied_2 = datasets.make_blobs(\n",
        "    n_samples=n_samples, cluster_std=[2.0, 1.4, 0.7], random_state=random_state\n",
        ")\n",
        "\n",
        "gaussian_quantile = datasets.make_gaussian_quantiles(random_state=random_state)\n",
        "\n",
        "# ============\n",
        "# Set up cluster parameters\n",
        "# ============\n",
        "plt.figure(figsize=(9 * 2 + 3, 13))\n",
        "plt.subplots_adjust(\n",
        "    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n",
        ")\n",
        "\n",
        "plot_num = 1\n",
        "\n",
        "default_base = {\n",
        "    \"quantile\": 0.3,\n",
        "    \"eps\": 0.3,\n",
        "    \"damping\": 0.9,\n",
        "    \"preference\": -200,\n",
        "    \"n_neighbors\": 3,\n",
        "    \"n_clusters\": 3,\n",
        "    \"min_samples\": 7,\n",
        "    \"xi\": 0.05,\n",
        "    \"min_cluster_size\": 0.1,\n",
        "    \"allow_single_cluster\": True,\n",
        "    \"hdbscan_min_cluster_size\": 15,\n",
        "    \"hdbscan_min_samples\": 3,\n",
        "    \"random_state\": 42,\n",
        "}\n",
        "\n",
        "datasets = [noisy_circles, circles, noisy_moons, noisy_moons_2, varied, varied_2, aniso, blobs, blobs_2, gaussian_quantile]\n",
        "\n",
        "for i_dataset, dataset in enumerate(datasets):\n",
        "    X, y = dataset\n",
        "    df = pd.DataFrame(X, columns=['feature_1', 'feature_2'])\n",
        "    if y is not None:\n",
        "        df['target'] = y\n",
        "    df.to_csv(f'dataset_{i_dataset}.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QET64cw8lcPg"
      },
      "source": [
        "**Distribuição normal multivariada**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCO9GRfyV19-"
      },
      "outputs": [],
      "source": [
        "# lista aleatória com as posições dos centros\n",
        "\n",
        "def generate_centers(num_centers, min_size, max_size, num_dimensions):\n",
        "    centers = []\n",
        "    for _ in range(num_centers):\n",
        "        size = np.random.randint(min_size, max_size + 1)\n",
        "        center = np.random.uniform(-10, 10, size=(size, num_dimensions))\n",
        "        centers.append(center)\n",
        "    return centers\n",
        "\n",
        "num_centers = 10\n",
        "min_size = 2\n",
        "max_size = 9\n",
        "num_dimensions = 2\n",
        "\n",
        "centers = generate_centers(num_centers, min_size, max_size, num_dimensions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfSis19Olp9l"
      },
      "outputs": [],
      "source": [
        "def generate_multivariate_data(centers, std_dev, num_samples_per_center):\n",
        "    data = []\n",
        "    for i, center in enumerate(centers):\n",
        "        samples = np.random.multivariate_normal(center, np.diag([std_dev, std_dev]), num_samples_per_center)\n",
        "        df_samples = pd.DataFrame(samples, columns=['X1', 'X2'])\n",
        "        df_samples['Center'] = i  # Adiciona um rótulo para identificar o centro\n",
        "        data.append(df_samples)\n",
        "\n",
        "    return pd.concat(data, ignore_index=True)\n",
        "\n",
        "import random\n",
        "\n",
        "std_dev_min = 0.5\n",
        "std_dev_max = 5.0\n",
        "std_dev_values = np.linspace(std_dev_min, std_dev_max, 10)\n",
        "num_samples_per_center = [random.randint(100, 200) for _ in range(10)]\n",
        "\n",
        "for i in range(10):\n",
        "  df = generate_multivariate_data(centers[i], std_dev_values[i], num_samples_per_center[i])\n",
        "  df.to_csv(f'dataset_{i}_dist_mult.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Lista para armazenar todos os DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Percorre todos os arquivos no diretório atual\n",
        "for file in os.listdir():\n",
        "    # Verifica se o arquivo termina com .csv\n",
        "    if file.endswith(\".csv\"):\n",
        "        # Lê o CSV em um DataFrame e adiciona à lista\n",
        "        df = pd.read_csv(file)\n",
        "        dataframes.append(df)\n",
        "\n",
        "# Concatena todos os DataFrames em um único DataFrame\n",
        "df_consolidado = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Salva o DataFrame consolidado em um novo arquivo CSV\n",
        "df_consolidado.to_csv('tabela_consolidada.csv', index=False)\n",
        "\n",
        "# Baixa o arquivo CSV consolidado\n",
        "from google.colab import files\n",
        "files.download('tabela_consolidada.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fNgbPHYvsaI4",
        "outputId": "848402d5-8109-476b-cfe6-b97c1db67f75"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7f08dd9f-14f6-4168-8a36-3000e8467d39\", \"tabela_consolidada.csv\", 5142)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Função para processar os arquivos CSV\n",
        "def processar_arquivos_csv():\n",
        "    for file in os.listdir():\n",
        "        # Verifica se o arquivo termina com .csv\n",
        "        if file.endswith(\".csv\"):\n",
        "            # Procura o padrão dentro dos colchetes no nome do arquivo\n",
        "            match = re.search(r'\\[(.*?)\\]', file)\n",
        "            if match:\n",
        "                # Extrai o valor dentro dos colchetes\n",
        "                valor = match.group(1)\n",
        "                # Lê o arquivo CSV\n",
        "                df = pd.read_csv(file)\n",
        "                # Modifica o conteúdo da primeira coluna, adicionando o valor extraído\n",
        "                df.iloc[:, 0] = df.iloc[:, 0].astype(str) + \"_\" + valor\n",
        "                # Salva o arquivo modificado\n",
        "                df.to_csv(file, index=False)\n",
        "            else:\n",
        "                # Se não houver colchetes, apenas lê e salva o arquivo sem modificações\n",
        "                df = pd.read_csv(file)\n",
        "                df.to_csv(file, index=False)\n",
        "\n",
        "# Executa a função para processar os arquivos CSV\n",
        "processar_arquivos_csv()\n"
      ],
      "metadata": {
        "id": "0AZ6cEQJuvnk"
      },
      "execution_count": 49,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}